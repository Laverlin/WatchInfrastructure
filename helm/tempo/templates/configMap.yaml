apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "tempo.fullname" . }}
  labels:
    {{- include "tempo.labels" . | nindent 4 }}
  namespace: {{ .Release.Namespace }}
data:
  allow-snippet-annotations: "false"
  tempo.yaml: |
    server:
      http_listen_port: {{ .Values.ports.tempoContainer }}

    distributor:
      receivers: 
        zipkin:
          endpoint: 0.0.0.0:9411
        otlp:
          protocols:
            http:
              endpoint: 0.0.0.0:{{ .Values.ports.oltpHttpContainer }}
            grpc:
              endpoint: 0.0.0.0:{{ .Values.ports.oltpGrpcContainer }}

    ingester:
      trace_idle_period: 10s               # the length of time after a trace has not received spans to consider it complete and flush it
      max_block_bytes: 1_000_000           # cut the head block when it hits this size or ...
      max_block_duration: 3m               #   this much time passes

    compactor:
      compaction:
        compaction_window: 1h              # blocks in this time window will be compacted together
        # max_block_bytes: 100000000       # maximum size of compacted blocks
        block_retention: 24h
        # compacted_block_retention: 10m

    storage:
      trace:
        backend: s3
        s3:
          endpoint: minio.database:9000
          region: ""
          access_key: tempoUser
          secret_key: tempoPwd
          bucket: tempo
          insecure: true
          forcepathstyle: true

        block:
          bloom_filter_false_positive: .05 # bloom filter false positive rate.  lower values create larger filters but fewer false positives

          v2_encoding: zstd                   # block encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2
        wal:
          path: /tmp/tempo/wal             # where to store the the wal locally
          v2_encoding: snappy                 # wal encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2

        pool:
          max_workers: 100                 # worker pool determines the number of parallel requests to the object store backend
          queue_depth: 10000
